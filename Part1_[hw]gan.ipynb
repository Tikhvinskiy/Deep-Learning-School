{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Копия блокнота \"[hw]gan.ipynb\"","provenance":[{"file_id":"1ahcV5rF0T1pwVxKes4cjp2bioZqa0xpO","timestamp":1623223757626},{"file_id":"1Zb0rRSCJkhuPL0IEPwbHmnwdEoX92BFa","timestamp":1622753094347}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xG34LB_ov1SV"},"source":["#### Tikhvinskiy, User ID: 136782149\n","\n","<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n","<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n","\n","<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n","\n","# Generative adversarial networks\n"]},{"cell_type":"markdown","metadata":{"id":"JEZSpS6zv5BP"},"source":["В этом домашнем задании вы обучите GAN генерировать лица людей и посмотрите на то, как можно оценивать качество генерации"]},{"cell_type":"markdown","metadata":{"id":"WrmSpt5e478V"},"source":["## Часть 1. Подготовка данных (1 балл)"]},{"cell_type":"markdown","metadata":{"id":"Dp2fR2Jd2eoh"},"source":["В качестве обучающей выборки возьмем часть датасета [Flickr Faces](https://github.com/NVlabs/ffhq-dataset), который содержит изображения лиц людей в высоком разрешении (1024х1024). Оригинальный датасет очень большой, поэтому мы возьмем его часть. Скачать датасет можно [здесь](https://drive.google.com/file/d/1KWPc4Pa7u2TWekUvNu9rTSO0U2eOlZA9/view?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"s0uiO3Za40iK"},"source":["Давайте загрузим наши изображения. Напишите функцию, которая строит DataLoader для изображений, при этом меняя их размер до нужного значения"]},{"cell_type":"code","metadata":{"id":"-7VrMX501uK0"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CHOdjsNW1Yxh","executionInfo":{"elapsed":7826,"status":"ok","timestamp":1623587041022,"user":{"displayName":"Andrey T","photoUrl":"","userId":"15869720638483276681"},"user_tz":-180},"outputId":"d6269377-ee81-499b-c8d5-bba78780388b"},"source":["# !unzip -q /content/gdrive/MyDrive/faces_dataset_small.zip -d ./faces\n","# !rm -r ./faces/__MACOSX\n","\n","# !unzip -q /content/gdrive/MyDrive/faces_dataset_128x128.zip -d ./faces\n","\n","!unzip -q /content/gdrive/MyDrive/faces_dataset_512x512.zip -d ./faces"],"execution_count":null,"outputs":[{"output_type":"stream","text":["unzip:  cannot find or open /content/gdrive/MyDrive/faces_dataset_512x512.zip, /content/gdrive/MyDrive/faces_dataset_512x512.zip.zip or /content/gdrive/MyDrive/faces_dataset_512x512.zip.ZIP.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"VceIWToY3250","executionInfo":{"elapsed":15,"status":"ok","timestamp":1623587041023,"user":{"displayName":"Andrey T","photoUrl":"","userId":"15869720638483276681"},"user_tz":-180},"outputId":"8ffcba55-8787-4cd6-a47a-3d91a08787b7"},"source":["!ls "],"execution_count":null,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"RK6xK2EwDMR0"},"source":["import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms as tt\n","import torch\n","import torch.nn as nn\n","import cv2\n","from tqdm.notebook import tqdm\n","from torchvision.utils import save_image\n","from torchvision.utils import make_grid\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import clear_output\n","import copy\n","%matplotlib inline\n","\n","sns.set(style='darkgrid', font_scale=1.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"PRqhM1VCwwYN"},"source":["def denorm(img_tensors):\n","    return img_tensors * stats[1][0] + stats[0][0]\n"," \n","\n","def norm(img_tensors):\n","    return (img_tensors - stats[0][0]) / stats[1][0] \n","\n","\n","def show_images(images, nmax=64):\n","    fig, ax = plt.subplots(figsize=(13, 13))\n","    ax.set_xticks([]); ax.set_yticks([])\n","    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n","    # ax.imshow(make_grid(images.detach()[:nmax], nrow=8).permute(1, 2, 0))\n","\n","def show_batch(dl, nmax=64):\n","    for images, _ in dl:\n","        show_images(images, nmax)\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CObqZZVdyyVg"},"source":["def get_dataloader(image_size, batch_size, stats, DATA_DIR):\n","  \"\"\"\n","  Builds dataloader for training data.\n","  Use tt.Compose and tt.Resize for transformations\n","  :param image_size: height and wdith of the image\n","  :param batch_size: batch_size of the dataloader\n","  :returns: DataLoader object \n","  \"\"\"\n","  # TODO: resize images, convert them to tensors and build dataloader\n","  train_ds = ImageFolder(DATA_DIR, transform=tt.Compose([\n","    # tt.Resize(image_size),\n","    # tt.CenterCrop(image_size),\n","    tt.ToTensor(),\n","    tt.Normalize(*stats)\n","    ]))\n","  train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","  return train_dl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"iwoDGf7myHPI","executionInfo":{"elapsed":12,"status":"ok","timestamp":1623604876124,"user":{"displayName":"Andrey T","photoUrl":"","userId":"15869720638483276681"},"user_tz":-180},"outputId":"2526cc27-d294-4c06-dc15-e7d9e16913e9"},"source":["DATA_DIR = './faces/'\n","image_size = 512\n","batch_size = 64\n","stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n","\n","latent_size = 1500 # choose latent size\n","lr = 0.0004 #0.0005 # 0.0002\n","epochs = 240\n","\n","\n","#TODO: build dataloader and transfer it to device\n","train_dl = get_dataloader(image_size, batch_size, stats, DATA_DIR)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a1a12529499a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#TODO: build dataloader and transfer it to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-9e7c87fa3198>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(image_size, batch_size, stats, DATA_DIR)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# tt.CenterCrop(image_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     ]))\n\u001b[1;32m     16\u001b[0m   \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    254\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    125\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './faces/'"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"S43d2VWxxv-j"},"source":["show_batch(train_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"fpB8pB4hyaBP"},"source":["def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","train_dl = DeviceDataLoader(train_dl, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"yKsmPiKYFvYY"},"source":["def mem_cuda(device=0):\n","    n = 1024 * 1024\n","    t = torch.cuda.get_device_properties(device).total_memory // n\n","    r = torch.cuda.memory_reserved(device) // n\n","    a = torch.cuda.memory_allocated(device) // n\n","    d = torch.cuda.get_device_name(device)\n","    f1 = t - (r + a)\n","    f2 = (r - a) # free inside reserved\n","    info = f'Device:{d}, all memory:{t}MB,  memory_reserved:{r}MB,  memory_allocated:{a}MB, free inside reserved:{f2}MB, free:{f1}MB'\n","    return info\n","\n","\n","torch.cuda.empty_cache()\n","mem_cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TgJiWnue5Aim"},"source":["## Часть 2. Построение и обучение модели (2 балла)"]},{"cell_type":"markdown","metadata":{"id":"n00W_EXg72er"},"source":["Сконструируйте генератор и дискриминатор. Помните, что:\n","* дискриминатор принимает на вход изображение (тензор размера `3 x image_size x image_size`) и выдает вероятность того, что изображение настоящее (тензор размера 1)\n","\n","* генератор принимает на вход тензор шумов размера `latent_size x 1 x 1` и генерирует изображение размера `3 x image_size x image_size`"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"zLMOs5O51BdB"},"source":["discriminator = nn.Sequential(\n","    # # in: 3 x 128 x 128\n","    # nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(64),\n","    # nn.LeakyReLU(0.2, inplace=True),\n","    # # out: 64 x 64 x 64\n","\n","    # nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(128),\n","    # nn.LeakyReLU(0.2, inplace=True),\n","    # # out: 128 x 32 x 32\n","\n","    # nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(256),\n","    # nn.LeakyReLU(0.2, inplace=True),\n","    # # out: 256 x 16 x 16\n","\n","    # nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(512),\n","    # nn.LeakyReLU(0.2, inplace=True),\n","    # # out: 512 x 8 x 8\n","\n","    # nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(1024),\n","    # nn.LeakyReLU(0.2, inplace=True),\n","    # # out: 1024 x 4 x 4\n","\n","    # nn.Conv2d(1024, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","    # # out: 1 x 1 x 1\n","\n","    # nn.Flatten(),\n","    # nn.Sigmoid())\n","\n","    # in: 3 x 512 x 512\n","    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(64),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    # out: 64 x 256 x 256\n","\n","    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(128),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    # out: 128 x 128 x 128\n","\n","    nn.Conv2d(128, 256, kernel_size=4, stride=4, padding=1, bias=False),\n","    nn.BatchNorm2d(256),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    # out: 256 x 32 x 32\n","\n","    nn.Conv2d(256, 512, kernel_size=4, stride=4, padding=1, bias=False),\n","    nn.BatchNorm2d(512),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    # out: 512 x 8 x 8\n","\n","    nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(1024),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    # out: 1024 x 4 x 4\n","\n","    nn.Conv2d(1024, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","    # out: 1 x 1 x 1\n","\n","    nn.Flatten(),\n","    nn.Sigmoid())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uZcIr8Ew4PRd"},"source":["discriminator = to_device(discriminator, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"7YXTW0PQ4Yfy"},"source":["generator = nn.Sequential(\n","    # # in: latent_size x 1 x 1\n","    # nn.ConvTranspose2d(latent_size, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n","    # nn.BatchNorm2d(1024),\n","    # nn.ReLU(True),\n","    # # out: 1024 x 4 x 4\n","\n","    # nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(512),\n","    # nn.ReLU(True),\n","    # # out: 512 x 8 x 8\n","\n","    # nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(256),\n","    # nn.ReLU(True),\n","    # # out: 256 x 16 x 16\n","\n","    # nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(128),\n","    # nn.ReLU(True),\n","    # # out: 128 x 32 x 32\n","\n","    # nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.BatchNorm2d(64),\n","    # nn.ReLU(True),\n","    # # out: 64 x 64 x 64\n","\n","    # nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n","    # # nn.Sigmoid())\n","    # nn.Tanh())\n","    # # out: 3 x 128 x 128\n","\n","\n","# in: latent_size x 1 x 1\n","\n","    nn.ConvTranspose2d(latent_size, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n","    nn.BatchNorm2d(1024),\n","    nn.ReLU(True),\n","    # out: 1024 x 4 x 4\n","\n","    nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=4, padding=0, bias=False),\n","    nn.BatchNorm2d(512),\n","    nn.ReLU(True),\n","    # out: 512 x 16 x 16\n","\n","    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=4, padding=0, bias=False),\n","    nn.BatchNorm2d(256),\n","    nn.ReLU(True),\n","    # out: 256 x 64 x 64\n","\n","    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(128),\n","    nn.ReLU(True),\n","    # out: 128 x 128 x 128\n","\n","    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(64),\n","    nn.ReLU(True),\n","    # out: 64 x 256 x 256\n","\n","    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n","    # nn.Sigmoid())\n","    nn.Tanh())\n","    # out: 3 x 512 x 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"m2zHAH265Rhf"},"source":["# посмотрим на выход генератора\n","xb = torch.randn(batch_size, latent_size, 1, 1) \n","print(xb.shape)\n","fake_images = generator(xb)\n","print(fake_images.shape)\n","show_images(fake_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"PkcBFD045ijx"},"source":["generator = to_device(generator, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"3amUjFgL1aGX"},"source":["def save_samples(index, latent_tensors, show=True, save=True, toColab=False):\n","    with torch.no_grad():\n","        fake_images = generator(latent_tensors)\n","        fake_fname = 'generated-images-{0:0=4d}.jpg'.format(index)\n","        if save and not toColab:\n","            save_image(denorm(fake_images)[:6], os.path.join(sample_dir, fake_fname), nrow=6)\n","            print('Saving', fake_fname)\n","        elif save and toColab:\n","            sample_dir = './gdrive/MyDrive/generatedGAN'\n","            save_image(denorm(fake_images)[:6], os.path.join(sample_dir, fake_fname), nrow=6)\n","            print('Saving', fake_fname)\n","\n","        if show:\n","            fig, ax = plt.subplots(figsize=(21, 5))\n","            ax.set_xticks([]); ax.set_yticks([])\n","            ax.imshow(make_grid(denorm(fake_images)[:6].cpu().detach(), nrow=6).permute(1, 2, 0))\n","            plt.show()\n","    \n","\n","sample_dir = 'generated'\n","os.makedirs(sample_dir, exist_ok=True)\n","\n","fixed_latent = torch.randn(batch_size, latent_size, 1, 1, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDHQaIzQ0B4S"},"source":["Перейдем теперь к обучению нашего GANа. Алгоритм обучения следующий:\n","1. Учим дискриминатор:\n","  * берем реальные изображения и присваиваем им метку 1\n","  * генерируем изображения генератором и присваиваем им метку 0\n","  * обучаем классификатор на два класса\n","\n","2. Учим генератор:\n","  * генерируем изображения генератором и присваиваем им метку 0\n","  * предсказываем дискриминаторором, реальное это изображение или нет\n","\n","\n","В качестве функции потерь берем бинарную кросс-энтропию"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"H2u0HPmk3B78"},"source":["model = {\n","    \"discriminator\": discriminator,\n","    \"generator\": generator\n","}\n","\n","criterion = {\n","    \"discriminator\": nn.BCELoss(),\n","    \"generator\": nn.BCELoss()\n","}\n","\n","optimizer = {\n","        \"discriminator\": torch.optim.Adam(model[\"discriminator\"].parameters(), \n","                                          lr=lr, betas=(0.5, 0.999)),\n","        \"generator\": torch.optim.Adam(model[\"generator\"].parameters(),\n","                                      lr=lr, betas=(0.5, 0.999))\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"Q_nMgY3w10EC"},"source":["def fit(model, criterion, epochs, lr, start_idx=1):\n","    print(mem_cuda())\n","    model[\"discriminator\"].train()\n","    model[\"generator\"].train()\n","    torch.cuda.empty_cache()\n","    \n","    # Losses & scores\n","    losses_g = []\n","    losses_d = []\n","    real_scores = []\n","    fake_scores = []\n","    next = 0\n","    # Create optimizers\n","    \n","    \n","    for epoch in range(epochs):\n","        loss_d_per_epoch = []\n","        loss_g_per_epoch = []\n","        real_score_per_epoch = []\n","        fake_score_per_epoch = []\n","        for real_images, _ in tqdm(train_dl):\n","            # Train discriminator\n","            # Clear discriminator gradients\n","            optimizer[\"discriminator\"].zero_grad()\n","\n","            # Pass real images through discriminator\n","            real_preds = model[\"discriminator\"](real_images)\n","            # real_targets = torch.ones(real_images.size(0), 1, device=device)\n","            real_targets = torch.FloatTensor(np.random.uniform(0.8, 1, (real_images.size(0), 1))).to(device)\n","            real_loss = criterion[\"discriminator\"](real_preds, real_targets)\n","            cur_real_score = torch.mean(real_preds).item()\n","            \n","            # Generate fake images\n","            latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n","            fake_images = model[\"generator\"](latent)\n","\n","            # Pass fake images through discriminator\n","            # fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n","            fake_targets = torch.FloatTensor(np.random.uniform(0, 0.2, (fake_images.size(0), 1))).to(device)\n","            fake_preds = model[\"discriminator\"](fake_images)\n","            fake_loss = criterion[\"discriminator\"](fake_preds, fake_targets)\n","            cur_fake_score = torch.mean(fake_preds).item()\n","\n","            real_score_per_epoch.append(cur_real_score)\n","            fake_score_per_epoch.append(cur_fake_score)\n","\n","            # Update discriminator weights\n","            loss_d = real_loss + fake_loss\n","            loss_d.backward()\n","            optimizer[\"discriminator\"].step()\n","            loss_d_per_epoch.append(loss_d.item())\n","\n","\n","            # Train generator\n","            # Clear generator gradients\n","            optimizer[\"generator\"].zero_grad()\n","            \n","            # Generate fake images\n","            latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n","            fake_images = model[\"generator\"](latent)\n","            \n","            # Try to fool the discriminator\n","            preds = model[\"discriminator\"](fake_images)\n","            targets = torch.ones(batch_size, 1, device=device)\n","            loss_g = criterion[\"generator\"](preds, targets)\n","            \n","            # Update generator weights\n","            loss_g.backward()\n","            optimizer[\"generator\"].step()\n","            loss_g_per_epoch.append(loss_g.item())\n","\n","        # каждую 60ю эпоху уменьшаем lr\n","        # if (epoch + 1) % 60 == 0:\n","        #     lr -= 0.0001\n","        #     optimizer[\"discriminator\"].param_groups[0]['lr'] = lr\n","        #     optimizer[\"generator\"].param_groups[0]['lr'] = lr\n","\n","        # каждую 10ю эпоху сохраняем модель\n","        if (epoch + 1) % 10 == 0:\n","            model_save = copy.deepcopy(model)\n","            torch.save(model_save, \"gdrive/MyDrive/model512_gan_e240.pth\")\n","            score_graph = losses_g + losses_d + real_scores + fake_scores\n","            torch.save(score_graph, \"gdrive/MyDrive/score_graph.pth\")\n","            \n","\n","        save_samples(epoch+start_idx, fixed_latent, show=False, save=True, toColab=True)\n","        \n","        # Record losses & scores\n","        losses_g.append(np.mean(loss_g_per_epoch))\n","        losses_d.append(np.mean(loss_d_per_epoch))\n","        real_scores.append(np.mean(real_score_per_epoch))\n","        fake_scores.append(np.mean(fake_score_per_epoch))\n","        \n","        opt_d, opt_g = optimizer[\"discriminator\"].param_groups[0]['lr'], optimizer[\"generator\"].param_groups[0]['lr']\n","\n","        clear_output(wait=True)\n","\n","        # Log losses & scores (last batch)\n","        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}, optimazer dis/gen={}/{}\".format(\n","            epoch+1, epochs, \n","            losses_g[-1], losses_d[-1], real_scores[-1], fake_scores[-1], \n","            opt_d, opt_g))\n","    \n","        # промежуточные результаты\n","        save_samples(epoch+start_idx, fixed_latent, show=True, save=False)\n","\n","        plt.figure(figsize=(20, 3))\n","        plt.plot(real_scores, label=\"real_scores\")\n","        plt.plot(fake_scores, label=\"fake_scores\")\n","        plt.legend(loc='best')\n","        plt.xlabel(\"epochs\")\n","        plt.ylabel(\"scores\")\n","        plt.show()  \n","        \n","        plt.figure(figsize=(20, 3))\n","        plt.plot(losses_g, label=\"losses_generator\")\n","        plt.plot(losses_d, label=\"losses_discriminator\")\n","        plt.legend(loc='best')\n","        plt.xlabel(\"epochs\")\n","        plt.ylabel(\"loss\")\n","        plt.show()  \n","        print(mem_cuda())\n","    \n","    return losses_g, losses_d, real_scores, fake_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"9zp7skLq6OEO"},"source":["history = fit(model, criterion, epochs, lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"aAKLFdK7UUFL"},"source":["# сохраним модель\n","\n","# model_save = copy.deepcopy(model)\n","# torch.save(model_save, \"gdrive/MyDrive/model_gan.pth\")\n","# model_save = copy.deepcopy(model)\n","# torch.save(model_save, \"gdrive/MyDrive/model512_gan.pth\")\n","\n","# model = torch.load(\"gdrive/MyDrive/model_gan.pth\")\n","# model = torch.load(\"gdrive/MyDrive/model512_gan_e240.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkecCSn69DLe"},"source":["Постройте графики лосса для генератора и дискриминатора. Что вы можете сказать про эти графики?\n","\n","Дискриминатор побеждает генератор. Хорошо распознает сгенерированные фотографии. "]},{"cell_type":"markdown","metadata":{"id":"kuL3ZZvX5G29"},"source":["## Часть 3. Генерация изображений (1 балл)"]},{"cell_type":"markdown","metadata":{"id":"7q9_WFIl-Bf6"},"source":["Теперь давайте оценим качество получившихся изображений. Напишите функцию, которая выводит изображения, сгенерированные нашим генератором"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"D-ZmT6qm4ai5"},"source":["def show_images(images):\n","    fig, ax = plt.subplots(figsize=(21, 10))\n","    ax.set_xticks([]); ax.set_yticks([])\n","    ax.imshow(make_grid(denorm(fake_images).cpu().detach(), nrow=10).permute(1, 2, 0))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"YKuZ1yNd8PLF"},"source":["n_images = 40\n","\n","fixed_latent = torch.randn(n_images, latent_size, 1, 1, device=device)\n","fake_images = model[\"generator\"](fixed_latent)\n","show_images(fake_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHqPK3xs-Z-7"},"source":["Как вам качество получившихся изображений?\n","\n","Качество изображений лучше чем генерирует автоинкодер."]},{"cell_type":"markdown","metadata":{"id":"c0z41dA05KAa"},"source":["## Часть 4. Leave-one-out-1-NN classifier accuracy (6 баллов)"]},{"cell_type":"markdown","metadata":{"id":"2C9V8DHX_ipy"},"source":["### 4.1. Подсчет accuracy (4 балла)"]},{"cell_type":"markdown","metadata":{"id":"9wT2uUb4_rku"},"source":["Не всегда бывает удобно оценивать качество сгенерированных картинок глазами. В качестве альтернативы вам предлагается реализовать следующий подход:\n","  * Сгенерировать столько же фейковых изображений, сколько есть настоящих в обучающей выборке. Присвоить фейковым метку класса 0, настоящим – 1.\n","  * Построить leave-one-out оценку: обучить 1NN Classifier (`sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`) предсказывать класс на всех объектах, кроме одного, проверить качество (accuracy) на оставшемся объекте. В этом вам поможет `sklearn.model_selection.LeaveOneOut`"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"tOPCVa-Quq76"},"source":["# для увеличения скорости обработки уменьшим размерность реальных картинок и синтезированных до 32х32х3\n","\n","del train_dl\n","size = 32\n","train_ds = ImageFolder(DATA_DIR, transform=tt.Compose([tt.Resize(size),tt.ToTensor(),tt.Normalize(*stats)]))\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"vsrgX9X4BfE0"},"source":["# добавим к реальным фото столько же синтезированных и вектор меток 1, 0\n","\n","n = len(train_dl)\n","real_pic = [0] * n\n","fake = [0] * n\n","for i, real_images in enumerate(train_dl):\n","      clear_output(wait=True)\n","      print('Batch', i + 1, 'of', n)\n","      real_pic[i] = real_images[0].permute(0, 2, 3, 1)\n","      latent = torch.randn(batch_size,latent_size, 1, 1, device=device)\n","      fake_gen = model[\"generator\"](latent).cpu().detach()\n","      fake[i] = tt.functional.resize(fake_gen, (size, size)).permute(0, 2, 3, 1)\n","\n","X = real_pic + fake\n","X = np.vstack([picts.numpy() for picts in X])\n","X = X.reshape(X.shape[0], -1)\n","y = np.array([1] * (X.shape[0] // 2) + [0] * (X.shape[0] // 2 + 1))\n","X.shape, y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"kJ-1Igb9X5T2"},"source":["%%time\n","# Построим leave-one-out оценку качества, для ускорения расчетов возьмем выборку изображений \n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.manifold import TSNE\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sample = 2000\n","mid = X.shape[0] // 2\n","X_ = X[mid - (sample // 2): mid + (sample // 2)]\n","y_ = y[mid - (sample // 2): mid + (sample // 2)]\n","\n","cf = KNeighborsClassifier(1)\n","score = cross_val_score(cf, X_, y_, scoring='accuracy', cv=LeaveOneOut(), n_jobs=-1, verbose=1).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"b4StAa6lx0gP"},"source":["print('Leave-one-out-1-NN classifier accuracy= ', score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jRU47nCzCVnP"},"source":["Что вы можете сказать о получившемся результате? Какой accuracy мы хотели бы получить и почему?\n","\n","Все-таки некоторые сгенерированные изображения классификатор не может отличить от настоящих. \n","Лучшим результатом было бы accuracy равным 0.5, что означало бы случайные совпадения(классификатор не может различить изображения). "]},{"cell_type":"markdown","metadata":{"id":"FqzHnPOACgoZ"},"source":["### 4.2. Визуализация распределений (2 балла)"]},{"cell_type":"markdown","metadata":{"id":"EweiItWFDYO0"},"source":["Давайте посмотрим на то, насколько похожи распределения настоящих и фейковых изображений. Для этого воспользуйтесь методом, снижающим размерность (к примеру, TSNE) и изобразите на графике разным цветом точки, соответствующие реальным и сгенерированным изображенияи"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"UZBJWkWdCepj"},"source":["%%time\n","X_embedded = TSNE(n_components=2).fit_transform(X)\n","labels = y\n","\n","plt.figure(figsize=(10, 8))\n","scatter = plt.scatter(X_embedded[:,0], X_embedded[:,1], c=labels, cmap='jet')\n","plt.legend(*scatter.legend_elements());"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVZe9tt8DuYh"},"source":["Прокомментируйте получившийся результат:\n","\n","Сгенерированные и изначальные изображения похожи и не имеют четких границ разделения."]}]}